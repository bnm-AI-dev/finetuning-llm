{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INKUBALM: FINETUNING AFRICA'S FIRST SMALL LANGUAGE MODEL\n",
    "\n",
    "The Lelapa AI Buzuzu-Mavi Challenge is a challenge to make the SLM released by Lelapa AI (Inkuba LM) smaller and smarter. Participants are able to improve their score by improving the average model performance, by making the model smaller, or by doing both. Improving average model performance involves making the model perform better for one or two languages across one or more tasks. The languages in question are Swahili and Hausa, and the tasks include sentiment analysis, AfriXNLI (true and false) question answering ability, and machine translation from English to the language in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-19T01:50:16.257696Z",
     "iopub.status.busy": "2025-07-19T01:50:16.257449Z",
     "iopub.status.idle": "2025-07-19T01:50:17.312550Z",
     "shell.execute_reply": "2025-07-19T01:50:17.311701Z",
     "shell.execute_reply.started": "2025-07-19T01:50:16.257675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/train-data/inkuba_combined_RAW.csv\n",
      "/kaggle/input/train-data/translation_train.csv\n",
      "/kaggle/input/train-data/train_Mega_v2.csv\n",
      "/kaggle/input/train-data/augmented_sentiment.csv\n",
      "/kaggle/input/train-data/augmented_translation.csv\n",
      "/kaggle/input/train-data/nli_train.csv\n",
      "/kaggle/input/train-data/inkuba_instruction_tuned_RAW.csv\n",
      "/kaggle/input/train-data/combined_aug_train.csv\n",
      "/kaggle/input/train-data/sentiment_train.csv\n",
      "/kaggle/input/train-data/augmented_qa.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T20:49:07.546169Z",
     "iopub.status.busy": "2025-04-01T20:49:07.545827Z",
     "iopub.status.idle": "2025-04-01T20:49:11.348077Z",
     "shell.execute_reply": "2025-04-01T20:49:11.346995Z",
     "shell.execute_reply.started": "2025-04-01T20:49:07.546139Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! pip install -U peft bitsandbytes accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T20:49:11.350251Z",
     "iopub.status.busy": "2025-04-01T20:49:11.349961Z",
     "iopub.status.idle": "2025-04-01T20:49:11.476598Z",
     "shell.execute_reply": "2025-04-01T20:49:11.475669Z",
     "shell.execute_reply.started": "2025-04-01T20:49:11.350220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "token = user_secrets.get_secret(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T20:49:11.478151Z",
     "iopub.status.busy": "2025-04-01T20:49:11.477880Z",
     "iopub.status.idle": "2025-04-01T20:49:11.499772Z",
     "shell.execute_reply": "2025-04-01T20:49:11.498882Z",
     "shell.execute_reply.started": "2025-04-01T20:49:11.478129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# read in the datasets\n",
    "base_dir = \"/kaggle/input/train-data/\"\n",
    "sentiment_df  = pd.read_csv(base_dir + \"sentiment_train.csv\")\n",
    "qa_df = pd.read_csv(base_dir + \"nli_train.csv\")\n",
    "translation_df = pd.read_csv(base_dir + \"translation_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T20:49:11.501172Z",
     "iopub.status.busy": "2025-04-01T20:49:11.500878Z",
     "iopub.status.idle": "2025-04-01T20:49:11.508785Z",
     "shell.execute_reply": "2025-04-01T20:49:11.507915Z",
     "shell.execute_reply.started": "2025-04-01T20:49:11.501136Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.task_templates = {\n",
    "            \"sentiment\": (\n",
    "                \"{instruction}\\n\"\n",
    "                \"{inputs}\\n\"\n",
    "                \"Output:\"\n",
    "            ),\n",
    "            \"mmt\": (\n",
    "                \"{instruction}\\n\"\n",
    "                \"{inputs}\\n\"\n",
    "                \"Output:\"\n",
    "            ),\n",
    "            \"nli\": (\n",
    "                \"{premise}\\n\"\n",
    "                \"{instruction}\\n\"\n",
    "                \"{inputs}\\n\"\n",
    "                \"Output:\"\n",
    "            )\n",
    "}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def build_prompt(self, example):\n",
    "        template = self.task_templates[example[\"task\"]]\n",
    "        return template.format(**example)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.data.iloc[idx]\n",
    "        prompt = self.build_prompt(example)\n",
    "        # Use 'targets' instead of 'label' since that's what's in the dataframe\n",
    "        full_text = prompt + \" \" + example[\"targets\"]\n",
    "\n",
    "        # Tokenize everything\n",
    "        tokenized = self.tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"][0]\n",
    "        attention_mask = tokenized[\"attention_mask\"][0]\n",
    "\n",
    "        # Create labels and apply loss masking\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Mask everything before the target\n",
    "        output_start = full_text.index(\"Output: \") + len(\"Output: \")\n",
    "        output_token_start = self.tokenizer(full_text[:output_start], return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        prefix_len = len(output_token_start)\n",
    "\n",
    "        labels[:prefix_len] = -100  # Ignore prompt in loss\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T20:49:11.509936Z",
     "iopub.status.busy": "2025-04-01T20:49:11.509656Z",
     "iopub.status.idle": "2025-04-01T20:49:14.287425Z",
     "shell.execute_reply": "2025-04-01T20:49:14.286667Z",
     "shell.execute_reply.started": "2025-04-01T20:49:11.509903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"lelapa/InkubaLM-0.4B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    token=token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T20:49:14.288505Z",
     "iopub.status.busy": "2025-04-01T20:49:14.288256Z",
     "iopub.status.idle": "2025-04-01T20:49:14.292542Z",
     "shell.execute_reply": "2025-04-01T20:49:14.291724Z",
     "shell.execute_reply.started": "2025-04-01T20:49:14.288482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Define task-specific configs\n",
    "# tasks = [\"sentiment\", \"mmt\", \"translation\"]\n",
    "\n",
    "# data_paths = {\n",
    "#     \"sentiment\": \"/kaggle/input/train-data/augmented_sentiment.csv\",\n",
    "#     \"nli\": \"/kaggle/input/train-data/augmented_qa.csv\",\n",
    "#     \"mmt\": \"/kaggle/input/train-data/augmented_translation.csv\"\n",
    "# }\n",
    "\n",
    "# for task in tasks:\n",
    "#     print(f\"\\nðŸ”§ Training LoRA adapter for: {task}\")\n",
    "\n",
    "#     # Create a fresh copy of the base model\n",
    "#     model = get_peft_model(base_model, LoraConfig(\n",
    "#         r=8,\n",
    "#         lora_alpha=16,\n",
    "#         target_modules=[\"q_proj\", \"v_proj\"],\n",
    "#         lora_dropout=0.05,\n",
    "#         bias=\"none\",\n",
    "#         task_type=\"CAUSAL_LM\"\n",
    "#     ))\n",
    "#     model.print_trainable_parameters()\n",
    "\n",
    "#     # Load and tokenize task-specific dataset\n",
    "#     df = pd.read_csv(data_paths[task])\n",
    "    \n",
    "#     dataset = InstructionDataset(df, tokenizer)\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=f\"/kaggle/working/outputs/lora_{task}_adapter\",\n",
    "#         per_device_train_batch_size=4,\n",
    "#         gradient_accumulation_steps=4,\n",
    "#         num_train_epochs=10,\n",
    "#         learning_rate=3e-4,\n",
    "#         fp16=True,\n",
    "#         logging_steps=20,\n",
    "#         save_strategy=\"no\",\n",
    "#         report_to=\"none\"\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=dataset\n",
    "#     )\n",
    "#     trainer.train()\n",
    "\n",
    "#     # Save task-specific adapter\n",
    "#     model.save_pretrained(f\"/kaggle/outputs/lora_{task}_adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T20:49:14.293739Z",
     "iopub.status.busy": "2025-04-01T20:49:14.293446Z",
     "iopub.status.idle": "2025-04-01T20:49:14.312318Z",
     "shell.execute_reply": "2025-04-01T20:49:14.311590Z",
     "shell.execute_reply.started": "2025-04-01T20:49:14.293707Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Detected 2 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "print(f\"ðŸš€ Detected {torch.cuda.device_count()} GPU(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T20:49:14.314553Z",
     "iopub.status.busy": "2025-04-01T20:49:14.314318Z",
     "iopub.status.idle": "2025-04-01T20:59:00.826673Z",
     "shell.execute_reply": "2025-04-01T20:59:00.825758Z",
     "shell.execute_reply.started": "2025-04-01T20:49:14.314532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Detected 2 GPU(s)\n",
      "\n",
      "ðŸ”§ Training LoRA adapter for: sentiment\n",
      "trainable params: 2,097,152 || all params: 424,036,352 || trainable%: 0.4946\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 03:13, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.728700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.135200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.297100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.082500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.849100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.733900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.729700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.510400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.199100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.430200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>3.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>3.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>3.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.950100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.926200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.874900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.706100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.653300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.546200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.523500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.512000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.522500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.304200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.159400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.353400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.316800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.095700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.333300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.319700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.097000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.324900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.079400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67ec520d-6ad8d9f10fceb4a06efafa25;46943fd9-1e1d-4b9e-afc5-90d16448f5ac)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/lelapa/InkubaLM-0.4B/resolve/main/config.json.\n",
      "Access to model lelapa/InkubaLM-0.4B is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in lelapa/InkubaLM-0.4B.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in lelapa/InkubaLM-0.4B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Training LoRA adapter for: mmt\n",
      "trainable params: 2,097,152 || all params: 424,036,352 || trainable%: 0.4946\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 03:14, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.496300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>8.342900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.342100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>7.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>7.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>7.188700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>7.129100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>6.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>7.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>7.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>6.877300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>6.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>6.833900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>7.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>6.741900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>6.931400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>6.744600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>6.930700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>6.955800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>6.705800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.861000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>7.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>6.757800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>6.556200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>6.706200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.803800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>6.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>6.769400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>6.630200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>6.781200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>6.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>6.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>6.749300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>6.747700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>6.484700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>6.750200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>6.782400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>6.597300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>6.517600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>6.513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>6.809100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>6.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>6.621300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>6.473100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>6.576200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.664200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67ec52d1-075ebaf646335f8c5845a929;a4c87175-f3cd-4be0-97b2-78a10a338eb5)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/lelapa/InkubaLM-0.4B/resolve/main/config.json.\n",
      "Access to model lelapa/InkubaLM-0.4B is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in lelapa/InkubaLM-0.4B.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in lelapa/InkubaLM-0.4B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Training LoRA adapter for: nli\n",
      "trainable params: 2,097,152 || all params: 424,036,352 || trainable%: 0.4946\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 03:14, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>6.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.960400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5.561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>5.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>5.361900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>5.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>5.369400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>4.985200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>5.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>5.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>5.114300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>4.741700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>4.984700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>4.970800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>4.693400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>4.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>4.831600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>4.530500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.928200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>4.771900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>4.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>4.681700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>4.625100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>4.506100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>4.627000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>4.320200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>4.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>4.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>4.329800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>4.553800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>4.505100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>4.501300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.260400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>4.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>4.564500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>4.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>4.171900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>4.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>4.592100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>4.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>4.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>4.597900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.284800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-67ec5394-28805ebb5f44b95730ce7425;dd47529c-bc2f-428b-b51c-3737ce615084)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/lelapa/InkubaLM-0.4B/resolve/main/config.json.\n",
      "Access to model lelapa/InkubaLM-0.4B is restricted. You must have access to it and be authenticated to access it. Please log in. - silently ignoring the lookup for the file config.json in lelapa/InkubaLM-0.4B.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in lelapa/InkubaLM-0.4B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "tasks = [\"sentiment\", \"mmt\", \"nli\"]\n",
    "\n",
    "data_paths = {\n",
    "    \"sentiment\": \"/kaggle/input/train-data/augmented_sentiment.csv\",\n",
    "    \"nli\": \"/kaggle/input/train-data/augmented_qa.csv\",\n",
    "    \"mmt\": \"/kaggle/input/train-data/augmented_translation.csv\"\n",
    "}\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"ðŸš€ Detected {torch.cuda.device_count()} GPU(s)\")\n",
    "\n",
    "for task in tasks:\n",
    "    print(f\"\\nðŸ”§ Training LoRA adapter for: {task}\")\n",
    "\n",
    "    # Create a fresh LoRA-wrapped model\n",
    "    model = get_peft_model(base_model, LoraConfig(\n",
    "        r=32,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    ))\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Load and tokenize dataset\n",
    "    df = pd.read_csv(data_paths[task])\n",
    "    dataset = InstructionDataset(df, tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"/kaggle/working/outputs/lora_{task}_adapter\",\n",
    "        per_device_train_batch_size=3,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=20,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=20,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_num_workers=2,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset\n",
    "    )\n",
    "\n",
    "    # \n",
    "    trainer.train()\n",
    "\n",
    "    # Save adapter weight(s)\n",
    "    model.save_pretrained(f\"/kaggle/working/lora_{task}_adapter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:00:38.241527Z",
     "iopub.status.busy": "2025-04-01T21:00:38.241110Z",
     "iopub.status.idle": "2025-04-01T21:00:40.098016Z",
     "shell.execute_reply": "2025-04-01T21:00:40.097133Z",
     "shell.execute_reply.started": "2025-04-01T21:00:38.241494Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): VulavulaLlamaForCausalLM(\n",
       "      (model): VulavulaLlamaModel(\n",
       "        (embed_tokens): Embedding(61788, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-7): 8 x VulavulaLlamaDecoderLayer(\n",
       "            (self_attn): VulavulaLlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): VulavulaLlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): VulavulaLlamaMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): VulavulaLlamaRMSNorm()\n",
       "            (post_attention_layernorm): VulavulaLlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): VulavulaLlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=61788, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=token)\n",
    "\n",
    "# Load adapter for a specific task\n",
    "task = \"sentiment\"  # or \"sentiment\", \"translation\"\n",
    "model = PeftModel.from_pretrained(base, f\"/kaggle/working/lora_{task}_adapter\")\n",
    "model.eval().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:00:45.360704Z",
     "iopub.status.busy": "2025-04-01T21:00:45.360404Z",
     "iopub.status.idle": "2025-04-01T21:00:47.173644Z",
     "shell.execute_reply": "2025-04-01T21:00:47.172901Z",
     "shell.execute_reply.started": "2025-04-01T21:00:45.360681Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Columns Index(['ID', 'task', 'langs', 'data_source', 'instruction', 'inputs',\n",
      "       'targets'],\n",
      "      dtype='object')\n",
      "NLI Columns Index(['ID', 'langs', 'premise', 'inputs', 'instruction', 'targets', 'task'], dtype='object')\n",
      "mmt Columns Index(['ID', 'task', 'langs', 'data_source', 'instruction', 'inputs',\n",
      "       'targets'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>task</th>\n",
       "      <th>langs</th>\n",
       "      <th>data_source</th>\n",
       "      <th>instruction</th>\n",
       "      <th>inputs</th>\n",
       "      <th>targets</th>\n",
       "      <th>premise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_f3c74c7b_sentiment_test__hausa</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>hausa</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>Gano ra'ayin da aka bayyana a cikin wannan rub...</td>\n",
       "      <td>@user ynxu fha da kanada kudi shikenan duk kay...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_aad19dbf_sentiment_test__hausa</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>hausa</td>\n",
       "      <td>naijasenti</td>\n",
       "      <td>Za ka iya tantance yanayin wannan rubutu? Bi w...</td>\n",
       "      <td>@user alhamdulillah babu abinda zamuce sai god...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_f6de0381_sentiment_test__hausa</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>hausa</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>Za ka iya tantance yanayin wannan rubutu? Bi w...</td>\n",
       "      <td>@user ke ina ruwan ki ðŸ˜¬ ba harkar film bane ba</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_cbec84fe_sentiment_test__swahili</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>swahili</td>\n",
       "      <td>afrisenti</td>\n",
       "      <td>Changanua mawazo ya matini yanayofuata na uain...</td>\n",
       "      <td>matokeo chanya liverais magufuli katika uzindu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_885caf5c_sentiment_test__hausa</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>hausa</td>\n",
       "      <td>naijasenti</td>\n",
       "      <td>Tantance raâ€™ayin wannan rubutu kuma a rarraba ...</td>\n",
       "      <td>@user ðŸ¤£ akwai lauje cikin nadi gaskiya.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    ID       task    langs data_source  \\\n",
       "0    ID_f3c74c7b_sentiment_test__hausa  sentiment    hausa   afrisenti   \n",
       "1    ID_aad19dbf_sentiment_test__hausa  sentiment    hausa  naijasenti   \n",
       "2    ID_f6de0381_sentiment_test__hausa  sentiment    hausa   afrisenti   \n",
       "3  ID_cbec84fe_sentiment_test__swahili  sentiment  swahili   afrisenti   \n",
       "4    ID_885caf5c_sentiment_test__hausa  sentiment    hausa  naijasenti   \n",
       "\n",
       "                                         instruction  \\\n",
       "0  Gano ra'ayin da aka bayyana a cikin wannan rub...   \n",
       "1  Za ka iya tantance yanayin wannan rubutu? Bi w...   \n",
       "2  Za ka iya tantance yanayin wannan rubutu? Bi w...   \n",
       "3  Changanua mawazo ya matini yanayofuata na uain...   \n",
       "4  Tantance raâ€™ayin wannan rubutu kuma a rarraba ...   \n",
       "\n",
       "                                              inputs  targets premise  \n",
       "0  @user ynxu fha da kanada kudi shikenan duk kay...      NaN     NaN  \n",
       "1  @user alhamdulillah babu abinda zamuce sai god...      NaN     NaN  \n",
       "2     @user ke ina ruwan ki ðŸ˜¬ ba harkar film bane ba      NaN     NaN  \n",
       "3  matokeo chanya liverais magufuli katika uzindu...      NaN     NaN  \n",
       "4            @user ðŸ¤£ akwai lauje cikin nadi gaskiya.      NaN     NaN  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_sent = pd.read_parquet(\"hf://datasets/lelapa/SentimentTest/data/train-00000-of-00001.parquet\")\n",
    "test_nli = pd.read_parquet(\"hf://datasets/lelapa/XNLITest/data/train-00000-of-00001.parquet\")\n",
    "test_mmt = pd.read_parquet(\"hf://datasets/lelapa/MTTest/data/train-00000-of-00001.parquet\")\n",
    "\n",
    "test_nli[\"task\"] = \"nli\"\n",
    "print(f\"Sentiment Columns {test_sent.columns}\")\n",
    "print(f\"NLI Columns {test_nli.columns}\")\n",
    "print(f\"mmt Columns {test_mmt.columns}\")\n",
    "\n",
    "test_data = pd.concat([test_sent, test_nli, test_mmt])\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:10:17.365654Z",
     "iopub.status.busy": "2025-04-01T21:10:17.365309Z",
     "iopub.status.idle": "2025-04-01T21:10:19.614416Z",
     "shell.execute_reply": "2025-04-01T21:10:19.613510Z",
     "shell.execute_reply.started": "2025-04-01T21:10:17.365626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Below is an instruction to complete a task. Make sure to produce a one word response or label. Do not give explanations or preamble.Tafadhali tambua mawazo yaliyoonyeshwa kwenye matini haya kwa kutegemea miongozo ifuatayo: Chanya: iwapo matini yanadokeza mawazo, mtazamo na hali chanya ya kihisia. Hasi: iwapo matini yanadokeza mawazo au hisia hasi. Wastani: iwapo matini hayadokezi lugha chanya au hasi kwa njia ya moja kwa moja au isiyo ya moja kwa moja.\n",
      "naomba kusema wazi kuwa 1 hedhi si jambo la ziada ambalo mwanamke anajiamulia iwe sehemu ya maisha yake au l\n",
      "Output:\n",
      "generated_response Wastani: Wastani: wanastani hichoacho ameutimizwa katika maji ya serengeti ili tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tatizo la maji tat\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate_response(prompt, max_new_tokens=128):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,  # Use greedy first\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    # Slice out the generated part only\n",
    "    generated = output_ids[0][input_ids.shape[1]:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "def build_prompt(row):\n",
    "    sent_nli_prompt = f\"Below is an instruction to complete a task. Make sure to produce a one word response or label. Do not give explanations or preamble.\"\n",
    "    if row[\"task\"] == \"sentiment\":\n",
    "        return sent_nli_prompt + f\"{row['instruction']}\\n{row['inputs']}\\nOutput:\"\n",
    "    elif row[\"task\"] == \"mmt\":\n",
    "        return f\"{row['instruction']}\\n{row['inputs']}\\nOutput:\"\n",
    "    elif row[\"task\"] == \"nli\":\n",
    "        return sent_nli_prompt + f\"{row['premise']}\\n{row['instruction']}\\n{row['inputs']}\\nOutput:\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task: {row['task']}\")\n",
    "\n",
    "\n",
    "row = test_data.iloc[40]\n",
    "prompt = build_prompt(row)\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"generated_response\", generate_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-01T23:44:51.163Z",
     "iopub.execute_input": "2025-04-01T21:10:35.886706Z",
     "iopub.status.busy": "2025-04-01T21:10:35.886403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "responses = []\n",
    "\n",
    "for _, row in test_data.iterrows():\n",
    "    task = row['task']\n",
    "    model = PeftModel.from_pretrained(base, f\"/kaggle/working/lora_{task}_adapter\")\n",
    "    model.eval().to(\"cuda\")\n",
    "    prompt = build_prompt(row)\n",
    "    response = generate_response(prompt)\n",
    "    responses.append({\n",
    "        \"ID\": row[\"ID\"],\n",
    "        \"task\": row[\"task\"],\n",
    "        \"Response\": response\n",
    "    })\n",
    "\n",
    "\n",
    "ss = pd.DataFrame(responses)\n",
    "ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-01T21:09:02.371455Z",
     "iopub.status.idle": "2025-04-01T21:09:02.371763Z",
     "shell.execute_reply": "2025-04-01T21:09:02.371641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def map_hausa_sent(response):\n",
    "    response = response.lower()\n",
    "    if \"kyakkyawa\" in response:\n",
    "        return 0\n",
    "    if \"tsaka\" in response:\n",
    "        return 1\n",
    "    if \"korau\" in response:\n",
    "        return 2\n",
    "        \n",
    "def map_swahili_sent(response):\n",
    "    response = response.lower()\n",
    "    if \"chanya\" in response:\n",
    "        return 0\n",
    "    if \"wastani\" in response:\n",
    "        return 1\n",
    "    if \"hasi\" in response:\n",
    "        return 2\n",
    "    \n",
    "def map_xnli(response):\n",
    "    response = response.lower()\n",
    "    if \"true\" in response:\n",
    "        return 0\n",
    "    if \"neutral\" in response:\n",
    "        return 1\n",
    "    if \"false\" in response:\n",
    "        return 2\n",
    "    if \"neither\" in response:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clean_labels = []\n",
    "for _, row in ss.iterrows():\n",
    "    response = row['Response']\n",
    "    task = row['task']\n",
    "    if task == \"sentiment\":\n",
    "        if \"swahili\" in row[\"ID\"]:\n",
    "            predicted_label = map_swahili_sent(response)\n",
    "            clean_labels.append(predicted_label)\n",
    "        if \"hausa\" in row[\"ID\"]:\n",
    "            predicted_label = map_hausa_sent(response)\n",
    "            clean_labels.append(predicted_label)\n",
    "    if task == \"nli\":\n",
    "        predicted_label = map_xnli(response)\n",
    "        clean_labels.append(predicted_label)\n",
    "    if task == \"mmt\":\n",
    "        clean_labels.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "clean_ss = pd.DataFrame({\n",
    "    \"ID\": ss['ID'].values,\n",
    "    \"Response\": clean_labels\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6914034,
     "sourceId": 11217754,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
